<h1>Método da Descida do Gradiente/h1>

<h2>Theory</h2>

<p align = "justify">
O Gradient Descent é um método usado na otimização para encontrar um mínimo (local) de uma função. O método utiliza um esquema iterativo no qual, a cada passo, é seguida a direção negativa do gradiente calculado. Podemos fazer uma analogia desse método com o ato de soltar um objeto do topo de uma montanha, de forma que o objeto desça até um ponto específico e pare; o ponto onde o objeto para é análogo ao mínimo local.
<br><br>
A equação abaixo descreve o que o algoritmo de gradiente descendente faz: \(x_{\text{new}}\) é a próxima posição do nosso "alpinista", enquanto \(x_{\text{old}}\) representa sua posição atual. O sinal de menos se refere à parte de minimização do algoritmo de gradiente descendente. O \(\eta\) no meio é a taxa de aprendizado, que controla o tamanho do passo na direção do gradiente, e o termo de gradiente \(\nabla f(x_{\text{old}})\) é simplesmente a direção da descida mais íngreme.
</p>

<table style = "width:100%">
    <tr>
        <td>\[x_{\text{new}} = x_{\text{old}} - \eta \cdot \nabla f(x_{\text{old}})\]
        <td><p align = "right">(1)</p></td>
    </tr>
</table>

<p align = "justify">