<!--Don't delete ths script-->
<script src = "https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id = "MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!--Don't delete ths script-->

<h1>GRADIENT DESCENDENTE</h1>

<h2>Theory</h2>
Gradient descent is a method used in optimization to find a (local) minimum of a function. The method employs an iterative scheme where, for each step, the negative direction of the calculated gradient is taken. We can make an analogy of this method to releasing an object from the top of a mountain, so that the object will descend to a certain point and stop; the point where the object stopped is analogous to the local minimum.
<p align = "justify">
<Br><Br>
The equation below describes what the gradient descent algorithm does: \ (x_{\text{new}}\) is the next position of our climber, while \(\x_{\text{old}}\) represents his current position. The minus sign refers to the minimization part of the gradient descent algorithm. The \( \eta \) in the middle is the learning rate, controlling the step size in the gradient direction and the gradient term \( \nabla f(x_{\text{old}}) \) is simply the direction of the steepest descent.
</p>

$$
x_{\text{new}} = x_{\text{old}} - \eta \cdot \nabla f(x_{\text{old}})


