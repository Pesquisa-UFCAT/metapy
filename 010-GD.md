<!--Don't delete ths script-->
<script src = "https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id = "MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!--Don't delete ths script-->

<h1>GRADIENT DESCENDENTE</h1>

<h2>Theory</h2>
Gradient descent is a method used in optimization to find a (local) minimum of a function. The method employs an iterative scheme where, for each step, the negative direction of the calculated gradient is taken. We can make an analogy of this method to releasing an object from the top of a mountain, so that the object will descend to a certain point and stop; the point where the object stopped is analogous to the local minimum.
<p align = "justify">
<Br><Br>
The equation below describes what the gradient descent algorithm does: \ (x_{\text{new}}\) is the next position of our climber, while \(\x_{\text{old}}\) represents his current position. The minus sign refers to the minimization part of the gradient descent algorithm. The \( \eta \) in the middle is the learning rate, controlling the step size in the gradient direction and the gradient term \( \nabla f(x_{\text{old}}) \) is simply the direction of the steepest descent.
</p>

$$
x_{\text{new}} = x_{\text{old}} - \eta \cdot \nabla f(x_{\text{old}})
$$
<Br><Br>
The equation above indicates that to obtain a new value of the parameters (\( x_{\text{new}} \)), we subtract the product of the learning rate (\( \eta \)) and the gradient of the function (\( \nabla f(x_{\text{old}}) \)) from the current value of the parameters (\( x_{\text{old}} \)).
<Br><Br>
The gradient (\( \nabla f(x_{\text{old}}) \)) is a vector pointing in the direction of the greatest function growth at the point \( x_{\text{old}} \). Multiplying the gradient by \(-1\) (i.e., taking the negative direction) indicates the direction of steepest descent, which is the direction we want to move in to decrease the function value.
<Br><Br>
The learning rate (\( \eta \)) is a positive value that determines the step size we take toward the minimum. Choosing the learning rate correctly is important; a too high rate can cause oscillations or divergence, while a too low rate can lead to slow convergence.
<Br><Br>
This equation is pivotal in gradient descent optimization, whether to find function minima or to adjust model parameters in machine learning algorithms.
